{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_raw_data(categories, remove):\n",
    "    train = fetch_20newsgroups(data_home='./data/',\n",
    "                               subset='train',\n",
    "                               categories=categories,\n",
    "                               remove=remove)\n",
    "    test = fetch_20newsgroups(data_home='./data/',\n",
    "                              subset='test',\n",
    "                              categories=categories,\n",
    "                              remove=remove)\n",
    "    X_train, y_train = train.data, train.target\n",
    "    X_test, y_test = test.data, test.target\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def clean_head_foot(docs):\n",
    "    '''\n",
    "    Remove header lines except 'Subject' and 'Organization'\n",
    "    '''\n",
    "    clean_docs = []\n",
    "    for doc in docs:\n",
    "        head, split, tail = doc.partition('\\n\\n')\n",
    "\n",
    "        # clean head\n",
    "        clean_head = '\\n'.join([line.strip().split(':')[-1]\n",
    "                                for line in head.strip().split('\\n')\n",
    "                                if line.strip().split(':')[0]\n",
    "                                in ('Subject', 'Organization')])\n",
    "\n",
    "        # remove foot\n",
    "        splited_tail = tail.strip().split('\\n')\n",
    "        for i in range(len(splited_tail) - 1, -1, -1):\n",
    "            if splited_tail[i] == '' or \\\n",
    "                    splited_tail[i].strip('-') == '' or \\\n",
    "                    splited_tail[i].strip('=') == '' or \\\n",
    "                    splited_tail[i].strip('#') == '' or \\\n",
    "                    splited_tail[i].strip('*') == '' or \\\n",
    "                    splited_tail[i].strip('\\n') == '':\n",
    "                break\n",
    "        clean_tail = '\\n'.join(splited_tail[:i])\n",
    "\n",
    "        clean_doc = clean_head + '\\n' + clean_tail\n",
    "        clean_docs.append(clean_doc)\n",
    "\n",
    "    return clean_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = fetch_raw_data(None, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = clean_head_foot(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " stolen CBR900RR\n",
      " California Institute of Technology, Pasadena\n",
      "Stolen from Pasadena between 4:30 and 6:30 pm on 4/15.\n",
      "\n",
      "Blue and white Honda CBR900RR california plate KG CBR.   Serial number\n",
      "JH2SC281XPM100187, engine number 2101240.\n",
      "\n",
      "No turn signals or mirrors, lights taped over for track riders session\n",
      "at Willow Springs tomorrow.  Guess I'll miss it.  :-(((\n",
      "\n",
      "Help me find my baby!!!\n"
     ]
    }
   ],
   "source": [
    "doc = X_train[-1]\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stolen',\n",
       " 'CBR900RR',\n",
       " 'California',\n",
       " 'Institute',\n",
       " 'of',\n",
       " 'Technology',\n",
       " 'Pasadena',\n",
       " 'Stolen',\n",
       " 'from',\n",
       " 'Pasadena',\n",
       " 'between',\n",
       " '4',\n",
       " '30',\n",
       " 'and',\n",
       " '6',\n",
       " '30',\n",
       " 'pm',\n",
       " 'on',\n",
       " '4',\n",
       " '15',\n",
       " 'Blue',\n",
       " 'and',\n",
       " 'white',\n",
       " 'Honda',\n",
       " 'CBR900RR',\n",
       " 'california',\n",
       " 'plate',\n",
       " 'KG',\n",
       " 'CBR',\n",
       " 'Serial',\n",
       " 'number',\n",
       " 'JH2SC281XPM100187',\n",
       " 'engine',\n",
       " 'number',\n",
       " '2101240',\n",
       " 'No',\n",
       " 'turn',\n",
       " 'signals',\n",
       " 'or',\n",
       " 'mirrors',\n",
       " 'lights',\n",
       " 'taped',\n",
       " 'over',\n",
       " 'for',\n",
       " 'track',\n",
       " 'riders',\n",
       " 'session',\n",
       " 'at',\n",
       " 'Willow',\n",
       " 'Springs',\n",
       " 'tomorrow',\n",
       " 'Guess',\n",
       " \"I'll\",\n",
       " 'miss',\n",
       " 'it',\n",
       " 'Help',\n",
       " 'me',\n",
       " 'find',\n",
       " 'my',\n",
       " 'baby']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('[\\w\\']+')\n",
    "tokens = tokenizer.tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
